Объединение сетевых интерфейсов в Linux
=======================================

[[!tag linux bonding]]

Содержание
----------

[[!toc startlevel=2 levels=4]]

Введение
--------

Объединение сетевых интерфейсов в единый интерфейс с целью повышения отказоустойчивости или использования суммарной пропускной способности двух и более сетевых интерфейсов в Linux осуществляется с помощью модуля ядра, который называется `bonding`. По названию драйвера само объединение сетевых интерфейсов в Linux тоже часто называют бондингом. В случае других сетевых устройств подобное объединение может называться агрегацией, "эзерчанелом" или "портчанелом" (от имени интерфейса Ether-Channel или Port-Channel на сетевом устройстве) и т.п.

Для загрузки модуля ядра достаточно ввести команду:

    # modprobe bonding

Для того, чтобы этот модуль ядра загружался автоматически при загрузке операционной системы в Debian нужно вписать имя модуля в файл `/etc/modules`.

Модуль может создавать несколько объединённых интерфейсов, каждый из которых может работать в одном из следующих режимов:

|Название     |Код|Отказоустойчивость|Балансировка нагрузки|Описание                                                                                                                                                                                                   |
|:------------|:-:|:----------------:|:-------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|balance-rr   | 0 |        Да        |         Да          |Отправка сетевых пакетов поочередно через все агрегированные интерфейсы (политика round-robin).                                                                                                            |
|active-backup| 1 |        Да        |         Нет         |Отправка сетевых пакетов через активный интерфейс. При отказе активного интерфейса (link down и т.д.) автоматически переключается на резервный интерфейс.                                                  |
|balance-xor  | 2 |        Да        |         Да          |Передача сетевых пакетов распределяются между интерфейсами на основе формулы (могут использоваться MAC-адрес, IP-адреса или номера IP-портов). Один и тот же интерфейс работает с определенным получателем.|
|broadcast    | 3 |        Да        |         Нет         |Отправка всех сетевых пакетов через все агрегированные интерфейсы (широковещательная отправка).                                                                                                            |
|802.3ad      | 4 |        Да        |         Нет         |Link Agregation Control Protocol, LACP - IEEE 802.3ad.                                                                                                                                                     |
|balance-tlb  | 5 |        Да        |         Да          |Входящие сетевые пакеты принимаются только активным сетевым интерфейсом, исходящие распределяется в зависимости от текущей загрузки каждого интерфейса.                                                    |
|balance-alb  | 6 |        Да        |         Да          |Исходящие сетевые пакеты распределяется между интерфейсами, входящие сетевые пакеты принимаются всеми интерфейсами.                                                                                        |

Примечания:

* Для использования режимов `balance-rr`, `balance-xor` и `broadcast` на коммутаторе сети должен быть настроено статическое объединение портов (static port trunking).
* Для использования режима `802.3ad` данный режим должен поддерживаться сетевым коммутатором.
* Для использования режима `balance-alb` требуются сетевые карты, поддерживающие смену MAC-адреса.

Ручная настройка
----------------

В разных дистрибутивах Linux используются разные системы управления сетевыми интерфейсами, которые, как правило, используют в своей работе пакет утилиту `ifenslave` из одноимённого пакета. Эта утилита может не устанавливаться по умолчанию, для её установки из сетевых репозиториев нужно сначала настроить сеть или прибегнуть к использованию флеш-накопителя и компьютера, уже подключенного к сети, что может оказаться неудобным. Может оказаться проще настроить сетевой интерфейс вручную, без использования утилиты `ifenslave`.

Рассмотрим для примера ручную настройку агрегации в режиме `802.3ad`.

Загрузим модуль ядра:

    # modprobe bonding

При загрузке модуля ядра можно сразу указать опции для будущих агрегирующих сетевых интерфейсов:

    # modprobe bonding mode=802.3ad xmit_hash_policy=layer2+3 miimon=100 lacp_rate=fast

В случае нескольких агрегирующих сетевых интерфейсов, можно подготовить псевдонимы модуля ядра и указать для каждого из псевдонимов модуля собственные настройки. Для этого необходимо добавить в каталог `/etc/modprobe.d` новые файлы с расширением `.conf`. Например, создадим файл `/etc/modprobe.d/bonding.conf` со следующим содержимым:

    alias bond0 bonding
    optinons bond0 mode=802.3ad xmit_hash_policy=layer2+3 miimon=100 lacp_rate=fast

В таком случае при попытке загрузить модуль ядра `bond0` с помощью приведённой ниже команды получится создать одноимённого сетевой интерфейс с указанными опциями:

    # modprobe bond0

Однако то же самое можно проделать и с помощью файловой системы `/sys/class/net/` описанным ниже образом.

Создадим агрегирующий интерфейс с именем `bond0`:

    # echo "+bond0" > /sys/class/net/bonding_masters

Переведём его в режим `802.3ad`:

    # echo "802.3ad" > /sys/class/net/bond0/bonding/mode

Настроим балансировку исходящего трафика с использованием MAC- и IP-адресов пакетов:

    # echo "layer2+3" > /sys/class/net/bond0/bonding/xmit_hash_policy

Настраиваем интервал проверки исправности связи (в миллисекундах):

    # echo "100" > /sys/class/net/bond0/bondign/miimon

Настраиваем частый обмен информацией между сторонами агрегации:

    # echo "fast" > /sys/class/net/bond0/bonding/lacp_rate

Добавляем в агрегацию сетевые интерфейсы `eno1` и `eno2`, не активируя их:

    # ip link set eno1 master bond0
    # ip link set eno2 master bond0

Активируем агрегирующий интерфейс:

    # ip link set bond0 up

Настраиваем на агрегирующем интерфейсе IP-адрес и, при необходимости, маршруты через него:

    # ip addr add 172.16.7.2/24 dev bond0
    # ip route add default dev bond0 via 172.16.7.1

Проверка состояния
------------------

Состояние агрегирующего сетевого интерфейса `bond0` можно узнать из файла `/proc/net/bonding/bond0`. Если не погружаться глубоко в рассмотрение содержимого файла, то исправность интерфейса можно проверить с помощью следующей команды:

    # fgrep Status /proc/net/bonding/bond0
    MII Status: up
    MII Status: up
    MII Status: up

Первая строчка соответствует состоянию агрегирующего сетевого интерфейса, а последующие - состоянию входящих в его состав агрегируемых сетевых интерфейсов. Если значение каждого из состояний соответствует тексту `up`, то агрегация полностью исправна.

Настройка в Debian
------------------

Для того, чтобы описанные выше ручные настройки восстанавливались в Debian при загрузке, нужно установить пакет `ifenslave`:

    # apt-get install ifenslave

И вписать настройки в файл `/etc/network/interfaces`:

    auto eno1
    iface eno1 inet manual
        bond-master bond0
    
    auto eno2
    iface eno2 inet manual
        bond-master bond0
    
    auto bond0
    iface bond0 inet static
        bond-slaves eno1 eno2
        bond-mode 802.3ad
        bond-xmit-hash-policy layer2+3
        bond-miimon 100
        bond-lacp-rate fast
        address 172.16.7.2/24
        gateway 172.16.7.1

Опции `bond-master` в объединяемых сетевых интерфейсах позволяют возвращать сетевой интерфейс в объединение, если зачем-то понадобится выключить и снова включить их с помощью команд `ifdown` и `ifup`:

    # ifdown eno1
    # ifup eno1

Если опцию `auto` заменить на `allow-hotplug`, то можно будет пользоваться демоном `ifplugd` для настройки сетевых интерфейсов после их физического исчезновения и возврата в систему, например, если это внешний USB-адаптер Ethernet.

Чтобы зафиксировать имена сетевых интерфейсов, можно создать файл `/etc/udev/rules.d/70-persistent-net.rules`, содержащий по одной строчке следующего вида для каждого из сетевых интерфейсов:

    SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:1e:67:5b:08:17", ATTR{dev_id}=="0x0", ATTR{type}=="1", KERNEL=="eth*", NAME="eno1"

Сетевому интерфейсу с MAC-адресом `00:1e:67:5b:08:17` будет назначено имя `eno1`.

Настройка в CentOS 6
--------------------

В случае с CentOS 6 утилита `ifenslave` входит в состав пакета `iputils`, который устанавливается по умолчанию при установке операционной системы.

В CentOS 6 конфигурация сетевых интерфейсов находится в отдельных файлах. Для того, чтобы при загрузке операционной системы восстанавливались настройки, заданные нами выше вручную, нужно отредактировать по одному файлу для каждого из сетевых интерфейсов. В примере ниже сетевые интерфейсы будут называться `eth1` и `eth2`.

Настройки сетевого интерфейса `eth1` впишем в файл конфигурации `/etc/sysconfig/network-scripts/ifcfg-eth1`:

    DEVICE=eth1
    ONBOOT=yes
    BOOTPROTO=none
    SLAVE=yes
    MASTER=bond0

Аналогичным образом настройки сетевого интерфейса `eth2` впишем в файл конфигурации `/etc/sysconfig/network-scripts/ifcfg-eth2`:

    DEVICE=eth2
    ONBOOT=yes
    BOOTPROTO=none
    SLAVE=yes
    MASTER=bond0

Наконец, настройки агрегирующего сетевого интерфейса `bond0` впишем в файл конфигурации `/etc/sysconfig/network-scripts/ifcfg-bond0`:

    DEVICE=bond0
    ONBOOT=yes
    BOOTPROTO=none
    IPADDR=172.16.7.2
    NETMASK=255.255.255.0
    GATEWAY=172.16.7.1
    BONDING_OPTS="mode=802.3ad xmit_hash_policy=layer2+3 miimon=100 lacp_rate=fast"

В данном случае настройки сетевого интерфейса соответствуют опциям, которые можно передать модулю ядра `bonding` при его ручной загрузке описанной выше командой `modprobe`.

Тестовый стенд 1
----------------

Для проверки правильности настройки агрегаций я решил воспользоваться двумя виртуальными машинами с Debian и CentOS. В качестве системы виртуализации я воспользовался KVM и графическим интерфейсом virt-manager, настроенными в соответствии со статьёй [[Настройка KVM и virt-manager в Debian 11|debian11_kvm]]. Предполагалось на каждой из виртуальных машин настроить по два сетевых интерфейса, объединённых в LACP. По плану предусматривалось настроить на системе виртуализации два сетевых моста и подключить к каждому из них по одному сетевому интерфейсу от каждой из виртуальных машин так, чтобы в итоге получилась такая схема:

    debian     |            host              | centos
      bond0    |                              |   bond0
        eno1 --|-- vnet1 -- virbr1 -- vnet3 --|---- eth1
        eno2 --|-- vnet2 -- virbr2 -- vnet4 --|---- eth2

Для начала настроим на системе в виртуализации два сетевых моста с именами `virbr1` и `virbr2`, для чего впишем в файл конфигурации `/etc/network/interfaces` следующие настройки:

    auto virbr1
    iface virbr1 inet manual
        bridge_ports none
        bridge_stp off
        bridge_fd 0
        bridge_maxwait 0
    
    auto virbr2
    iface virbr2 inet manual
        bridge_ports none
        bridge_stp off
        bridge_fd 0
        bridge_maxwait 0

Активируем эти сетевые интерфейсы при помощи следующих команд:

    # ifup virbr1
    # ifup virbr2

На виртуальных машинах настроим сетевые интерфейсы следующим образом:

[[vm_nic.png]]

С помощью команды следующего вида можно посмотреть или отредактировать XML-конфигурацию виртуальной машины:

    # virsh edit vm

Фрагмент XML-конфигурации, соответствующий сетевому интерфейсу, выглядит следующим образом:

    <interface type='bridge'>
      <mac address='52:54:00:16:5e:3b'/>
      <source bridge='virbr1'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
    </interface>

К сожалению, на этом тестовом стенде агрегация сетевых интерфейсов работала загадочно. При неактивном сетевом интерфейсе `virbr2` связь между виртуальными машинами оставалась, а вот при неактивном сетевом интерфейсе `virbr1` связь пропадала.

На виртуальной машине с CentOS, судя по содержимому файла `/proc/net/bonding/bond0`, не происходило согласование агрегации по протоколу LACP с партнёром по агрегации - виртуальной машиной с Debian.

На виртуальной машине с Debian можно было наблюдать с виду более благоприятную картину, однако при ближайшем рассмотрении оказывалось, что второй сетевой интерфейс имеет идентификатор, отличный от интерфейсов агрегирующего интерфейса и первого интерфейса, входящего в агрегацию:

    # fgrep 'Aggregator ID' /proc/net/bonding/bond0
            Aggregator ID: 1
    Aggregator ID: 1
    Aggregator ID: 2

При исправно работающей согласованной по протоколу LACP агрегации, картина должна была бы выглядеть следующим образом:

    # fgrep 'Aggregator ID' /proc/net/bonding/bond0
    Aggregator ID: 1
    Aggregator ID: 1

Было подозрение, что сетевые мосты не транслируют трафик протокола LACP. В пользу этой версии говорило наблюдение за трафиком LACP на сетевых интерфейсах vnet и virbr с помощью `tcpdump`:

    # tcpdump -npi vnet61 -e ether proto 0x8809

На сетевых интерфейсах vnet можно было наблюдать попытки отправить пакет по протоколу LACP на мультикаст-адрес, в то время как на мостовых интерфейсах virbr подобный трафик отсутствовал:

    16:16:59.047875 fe:54:00:16:5e:3b > 01:80:c2:00:00:02, ethertype Slow Protocols (0x8809), length 124: LACPv1, length 110

Использованные материалы
------------------------

* [Механизмы агрегации сетевых каналов](https://wiki.astralinux.ru/pages/viewpage.action?pageId=158604474)
* [Preparing a bonded interface](https://www.ibm.com/docs/en/linux-on-systems?topic=connection-bonded-interface)
* [man interfaces-bond(5)](https://manpages.debian.org/testing/ifupdown-ng/interfaces-bond.5.en.html)
* [Linux bonding — объединение сетевых интерфейсов в Linux](https://www.adminia.ru/linux-bonding-obiedinenie-setevyih-interfeysov-v-linux/)
